# RET v4 - Complete Features Documentation

**Version**: 4.0.0  
**Date**: January 27, 2026  
**Status**: Fully Integrated and Ready for Testing

---

## ğŸ“‹ Table of Contents

1. [Application Overview](#application-overview)
2. [Architecture](#architecture)
3. [Core Features](#core-features)
4. [API Endpoints](#api-endpoints)
5. [Frontend Components](#frontend-components)
6. [Database Schema](#database-schema)
7. [External Integrations](#external-integrations)
8. [Testing Overview](#testing-overview)

---

## Application Overview

### What is RET v4?

RET (Rapid Extraction Tool) v4 is a comprehensive web application for:
- **XML Processing**: Upload ZIP files containing XML documents, automatically detect structure
- **Format Conversion**: Convert XML to CSV/XLSX with group-based organization
- **File Comparison**: Compare two files with field-level delta analysis
- **AI-Powered Search**: Use Azure OpenAI with Chroma DB for semantic search and Q&A on uploaded documents

### Technology Stack

**Backend**:
- Framework: FastAPI (Python 3.12+)
- Database: PostgreSQL
- Vector DB: Chroma (for RAG)
- Embeddings: Azure OpenAI Embeddings
- Chat Model: Azure OpenAI GPT-4.1
- Job Queue: Celery + Redis
- ORM: SQLAlchemy

**Frontend**:
- Framework: Vue 3 with `<script setup>`
- Build Tool: Vite
- CSS: Custom with component styles
- State Management: Pinia
- HTTP Client: Axios
- Routing: Vue Router

**Infrastructure**:
- Authentication: JWT tokens (stored in memory + HttpOnly cookies)
- CORS: Properly configured for local development
- Session Management: Per-user session directories
- File Storage: Local filesystem

---

## Architecture

### High-Level Flow

```
User Browser
    â†“
Frontend (Vue 3)
    â†“ HTTP/CORS
API Gateway (FastAPI)
    â†“
Routers & Schemas
    â†“
Services Layer
    â”œâ†’ Auth Service
    â”œâ†’ Conversion Service
    â”œâ†’ Comparison Service
    â”œâ†’ AI Indexing Service
    â”œâ†’ AI Chat Service
    â”œâ†’ Storage Service
    â””â†’ Job Service
    â†“
Database Layer
    â”œâ†’ PostgreSQL (users, jobs, etc.)
    â”œâ†’ Chroma DB (vector embeddings per session)
    â””â†’ File Storage (sessions/{session_id}/)
    â†“
External Services
    â”œâ†’ Azure OpenAI Embeddings API
    â””â†’ Azure OpenAI Chat API
```

### Session Isolation

Each user session is isolated:

```
User Login
    â†“
Create Session Directory: runtime/sessions/{session_id}/
    â”œâ”€â”€ extracted/        (ZIP contents)
    â”œâ”€â”€ output/           (Converted CSVs)
    â”œâ”€â”€ ai_index/         (Chroma DB - per session)
    â”œâ”€â”€ metadata.json     (Session info)
    â””â”€â”€ job_log.txt       (Job history)
    
User Logout
    â†“
Cleanup: Delete entire sessions/{session_id}/ directory
```

---

## Core Features

### Feature 1: Authentication & Authorization

**Endpoints**:
- `POST /api/auth/login` - Login with credentials
- `POST /api/auth/refresh` - Refresh expired access token
- `GET /api/auth/me` - Get current user info
- `POST /api/auth/logout` - Logout and cleanup
- `POST /api/auth/password-reset` - Reset password

**How It Works**:
1. User submits username/password to `/api/auth/login`
2. Backend validates credentials against PostgreSQL
3. Backend generates JWT tokens (short-lived access_token, long-lived refresh_token)
4. Frontend stores access_token in memory (not localStorage - more secure)
5. refresh_token stored in HttpOnly cookie (automatic, cannot access via JS)
6. All API requests include `Authorization: Bearer {access_token}` header
7. When token expires (401 response), frontend automatically calls `/api/auth/refresh`
8. Backend validates refresh_token, issues new access_token
9. Original request is retried
10. On logout, all session data is deleted

**Security Features**:
- âœ… Passwords hashed with Argon2
- âœ… JWT with HS256 signing
- âœ… HttpOnly cookies prevent XSS
- âœ… CSRF protection via CORS
- âœ… Automatic token rotation
- âœ… Session timeout after 30 minutes

---

### Feature 2: File Upload & ZIP Processing

**Endpoints**:
- `POST /api/conversion/scan` - Upload and scan ZIP file

**How It Works**:

```
1. User selects ZIP file from Examples folder
2. File uploaded to backend (multipart/form-data)
3. ZIP extracted and validated:
   â”œâ”€â”€ Check for bomb attacks (compression ratio)
   â”œâ”€â”€ Scan for .xml files
   â”œâ”€â”€ Extract to: runtime/sessions/{session_id}/extracted/
4. Detect groups from:
   â”œâ”€â”€ Directory structure (folder names)
   â”œâ”€â”€ File name patterns (prefix matching)
   â””â”€â”€ XML root element analysis
5. Return to frontend:
   {
     "session_id": "uuid-xxx",
     "groups": [
       {"name": "JOURNAL", "count": 5},
       {"name": "BOOK", "count": 2}
     ],
     "total_files": 7,
     "extracted_path": "/app/runtime/sessions/xxx/extracted"
   }
```

**Group Detection Algorithm**:
- Extracts common prefixes from file paths
- Normalizes to uppercase (e.g., "journal_article_4.4.2.xml" â†’ "JOURNAL")
- Groups files by detected prefix
- Returns list of unique groups with file counts

**Example**:
```
Input ZIP contains:
  journal_article_1.xml â†’ Group: JOURNAL
  journal_article_2.xml â†’ Group: JOURNAL
  book_chapter_1.xml â†’ Group: BOOK
  dissertation_ch1.xml â†’ Group: DISSERTATION

Output:
  JOURNAL (2 files)
  BOOK (1 file)
  DISSERTATION (1 file)
```

---

### Feature 3: XML to CSV/XLSX Conversion

**Endpoints**:
- `POST /api/conversion/convert` - Start async conversion job
- `GET /api/jobs/{job_id}` - Check job status
- `GET /api/conversion/download/{session_id}` - Download results

**How It Works**:

```
1. User selects groups to convert
2. Frontend calls POST /api/conversion/convert with:
   {
     "session_id": "...",
     "groups": ["JOURNAL", "BOOK"],
     "output_format": "csv"
   }

3. Backend creates Job record in PostgreSQL
4. Enqueues Celery task: conversion_task
5. Returns job_id to frontend

6. Conversion Worker (async):
   â”œâ”€â”€ Load XML files for each group
   â”œâ”€â”€ Parse XML using LXML
   â”œâ”€â”€ For each group:
   â”‚   â”œâ”€â”€ Find record elements (e.g., <article>, <book>)
   â”‚   â”œâ”€â”€ Flatten each element to CSV row
   â”‚   â”œâ”€â”€ Extract all fields (nested elements become columns)
   â”‚   â””â”€â”€ Write to CSV: output/{GROUP_NAME}.csv
   â”œâ”€â”€ Update job status to "completed"
   â””â”€â”€ Store output files

7. Frontend polls /api/jobs/{job_id}:
   â”œâ”€â”€ Checks status every 1-2 seconds
   â”œâ”€â”€ When status = "completed"
   â”œâ”€â”€ Shows Download button
   
8. Download via /api/conversion/download/{session_id}:
   â””â”€â”€ Returns ZIP with all group CSVs
```

**XML Flattening Example**:

```xml
Input:
<article>
  <id>DOI-001</id>
  <title>Article Title</title>
  <author>
    <name>John Doe</name>
    <affiliation>University</affiliation>
  </author>
  <publication>
    <date>2025-01-20</date>
  </publication>
</article>

Output CSV Headers:
id, title, author_name, author_affiliation, publication_date

Output CSV Row:
DOI-001, Article Title, John Doe, University, 2025-01-20
```

---

### Feature 4: File Comparison with Delta Analysis

**Endpoints**:
- `POST /api/comparison/run` - Compare two files
- `POST /api/comparison/compare` - Async batch comparison

**How It Works**:

```
1. User uploads File A (CSV/JSON) and File B (CSV/JSON)
2. Frontend calls POST /api/comparison/run with multipart files
3. Backend:
   â”œâ”€â”€ Parse both files (detect format)
   â”œâ”€â”€ Match rows between files (by ID or position)
   â”œâ”€â”€ For each row pair:
   â”‚   â”œâ”€â”€ Compare field-by-field
   â”‚   â”œâ”€â”€ Mark changed fields with ğŸŸ¢
   â”‚   â”œâ”€â”€ Mark unchanged fields with ğŸ”´
   â””â”€â”€ Generate statistics:
       â”œâ”€â”€ Added rows (in B but not A)
       â”œâ”€â”€ Removed rows (in A but not B)
       â”œâ”€â”€ Modified rows (different values)
       â””â”€â”€ Similarity percentage (SequenceMatcher)

4. Return detailed delta report:
   {
     "similarity_percentage": 87.5,
     "total_rows_a": 50,
     "total_rows_b": 52,
     "added_rows": 3,
     "removed_rows": 1,
     "modified_rows": 5,
     "field_changes": [
       {
         "row_id": 1,
         "field": "title",
         "value_a": "Old Title",
         "value_b": "New Title",
         "changed": true,
         "indicator": "ğŸŸ¢"
       }
     ]
   }
```

**Delta Indicators**:
- ğŸŸ¢ **Green Dot**: Field value changed (significant)
- ğŸ”´ **Red Dot**: Field value unchanged (expected)
- Added Rows: New entries in File B
- Removed Rows: Deleted entries from File A
- Modified Rows: Changed data in existing rows

---

### Feature 5: AI Indexing - Create Local Chroma DB

**Endpoints**:
- `POST /api/ai/index` - Index groups to Chroma
- `GET /api/ai/indexed-groups/{session_id}` - List indexed groups
- `POST /api/ai/clear/{session_id}` - Clear session memory

**How It Works** (RAG Preparation):

```
1. User selects groups to index:
   POST /api/ai/index
   {
     "session_id": "...",
     "groups": ["JOURNAL", "BOOK"]
   }

2. Backend Indexing Process:
   â”œâ”€â”€ Create Chroma DB: runtime/sessions/{session_id}/ai_index/
   â”‚
   â”œâ”€â”€ For each selected group:
   â”‚   â”œâ”€â”€ Load XML files from extracted/
   â”‚   â”œâ”€â”€ Extract text content from each file:
   â”‚   â”‚   â”œâ”€â”€ Parse XML
   â”‚   â”‚   â”œâ”€â”€ Get title, abstract, content
   â”‚   â”‚   â””â”€â”€ Flatten nested elements
   â”‚   â”‚
   â”‚   â”œâ”€â”€ Create chunks (max 800 characters):
   â”‚   â”‚   â”œâ”€â”€ Chunk 1: [0-800 chars]
   â”‚   â”‚   â”œâ”€â”€ Chunk 2: [800-1600 chars]
   â”‚   â”‚   â””â”€â”€ Chunk N: [...]
   â”‚   â”‚
   â”‚   â”œâ”€â”€ Call Azure OpenAI Embeddings API:
   â”‚   â”‚   â””â”€â”€ embed_text() for each chunk
   â”‚   â”‚   â””â”€â”€ Returns 1536-dim vector
   â”‚   â”‚
   â”‚   â”œâ”€â”€ Store in Chroma DB:
   â”‚   â”‚   â”œâ”€â”€ Document: chunk text
   â”‚   â”‚   â”œâ”€â”€ Embedding: vector from API
   â”‚   â”‚   â”œâ”€â”€ Metadata: {source: "file.xml", group: "JOURNAL"}
   â”‚   â”‚   â””â”€â”€ ID: "group-chunk-001"
   â”‚
   â””â”€â”€ Return stats:
       {
         "indexed_groups": ["JOURNAL", "BOOK"],
         "total_documents": 1250,
         "message": "Indexed 2 groups with 1250 documents"
       }

3. Session Memory:
   â”œâ”€â”€ Stored in: runtime/sessions/{session_id}/ai_index/
   â”œâ”€â”€ Isolated from other sessions
   â”œâ”€â”€ Deleted on logout
```

**Embedding Details**:
- **API**: Azure OpenAI Text Embedding
- **Model**: text-embedding-3-large (1536 dimensions)
- **Cost**: Per 1M tokens (~$0.02)
- **Time**: ~2-5 seconds for typical document
- **Quality**: Semantic similarity (high precision)

**Example**:
```
Text Chunk:
"The article discusses machine learning applications in 
healthcare. Authors from MIT and Stanford collaborated on 
this research, published in 2024."

Embedding API Call:
POST https://btgazureopenai.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings
Headers: api-key: {key}
Body: {"input": "The article discusses..."}

Response:
{
  "data": [{
    "embedding": [0.123, -0.456, 0.789, ..., 0.321],  // 1536 values
    "index": 0
  }]
}

Chroma DB Storage:
{
  "id": "JOURNAL-chunk-001",
  "document": "The article discusses...",
  "embedding": [0.123, -0.456, 0.789, ..., 0.321],
  "metadata": {
    "source": "journal_article_1.xml",
    "group": "JOURNAL",
    "chunk": 1
  }
}
```

---

### Feature 6: AI Chat with RAG (Retrieval-Augmented Generation)

**Endpoints**:
- `POST /api/ai/chat` - Ask question with RAG

**How It Works** (Core AI Feature):

```
1. User asks question on frontend:
   "What is the main topic discussed?"

2. Frontend calls:
   POST /api/ai/chat
   {
     "session_id": "...",
     "question": "What is the main topic discussed?"
   }

3. Backend RAG Pipeline:

   Step A: Embed User Question
   â”œâ”€â”€ Call Azure OpenAI Embeddings
   â”œâ”€â”€ Get vector representation of question
   â””â”€â”€ Result: 1536-dim vector

   Step B: Semantic Search in Chroma
   â”œâ”€â”€ Query Chroma DB with question vector
   â”œâ”€â”€ Find top-5 most similar documents (by cosine similarity)
   â”œâ”€â”€ Calculate similarity scores (0-1)
   â””â”€â”€ Results: [
         {chunk: "...", score: 0.92, source: "journal_1.xml"},
         {chunk: "...", score: 0.87, source: "journal_2.xml"},
         ...
       ]

   Step C: Build Context
   â”œâ”€â”€ Concatenate top-5 chunks
   â”œâ”€â”€ Create system prompt:
   â”‚   "You are RET AI assistant. Answer using provided context."
   â”œâ”€â”€ Create user prompt:
   â”‚   "Context:\n[chunks from Step B]\n\nQuestion: [user question]"

   Step D: Get AI Answer
   â”œâ”€â”€ Call Azure OpenAI Chat (gpt-4.1)
   â”œâ”€â”€ System: "You are RET AI assistant..."
   â”œâ”€â”€ User: "Context:\n...\n\nQuestion: What is the main topic?"
   â””â”€â”€ Response: "Based on the indexed documents, the main topic is..."

   Step E: Format Response
   â””â”€â”€ Return with citations:
       {
         "answer": "Based on the documents, the main topic is...",
         "citations": [
           {
             "document": "[chunk 1 text]",
             "source": "journal_article_1.xml",
             "similarity_score": 0.92
           },
           ...
         ]
       }

4. Frontend displays:
   â”œâ”€â”€ Answer text
   â”œâ”€â”€ Citation sources with similarity scores
   â””â”€â”€ Allow follow-up questions
```

**RAG Benefits**:
- âœ… Answers grounded in actual documents
- âœ… No hallucinations (uses only provided context)
- âœ… Cites sources for verification
- âœ… Works with any domain-specific documents
- âœ… Up-to-date information (from your data)
- âœ… No retraining required

**Example RAG Response**:
```
User Question:
"What methodologies were used in the research?"

RAG Process:
1. Embed question
2. Search Chroma DB â†’ Find top-5 matching chunks
3. Retrieved context:
   - Chunk A (score 0.94): "The study employed quantitative analysis..."
   - Chunk B (score 0.91): "Data was collected through surveys..."
   - Chunk C (score 0.88): "Statistical tests included ANOVA..."
   
4. Call GPT-4 with context + question
5. GPT-4 responds: "Based on the research documents, 
   the methodologies included quantitative analysis (score 0.94), 
   survey-based data collection (score 0.91), and statistical 
   tests including ANOVA (score 0.88)..."

Citations shown to user:
- [âœ“ 0.94] "The study employed quantitative analysis..."
- [âœ“ 0.91] "Data was collected through surveys..."
- [âœ“ 0.88] "Statistical tests included ANOVA..."
```

---

## API Endpoints

### Authentication Endpoints

| Method | Endpoint | Purpose | Auth Required |
|--------|----------|---------|---|
| POST | `/api/auth/login` | User login | âŒ |
| POST | `/api/auth/refresh` | Refresh token | âŒ (uses cookie) |
| GET | `/api/auth/me` | Get user info | âœ… |
| POST | `/api/auth/logout` | Logout user | âœ… |

### Conversion Endpoints

| Method | Endpoint | Purpose | Auth Required |
|--------|----------|---------|---|
| POST | `/api/conversion/scan` | Scan ZIP file | âœ… |
| POST | `/api/conversion/convert` | Start conversion job | âœ… |
| GET | `/api/conversion/download/{session_id}` | Download CSV | âœ… |

### Comparison Endpoints

| Method | Endpoint | Purpose | Auth Required |
|--------|----------|---------|---|
| POST | `/api/comparison/run` | Compare files | âœ… |

### AI Endpoints

| Method | Endpoint | Purpose | Auth Required |
|--------|----------|---------|---|
| POST | `/api/ai/index` | Index groups to Chroma | âœ… |
| GET | `/api/ai/indexed-groups/{session_id}` | List indexed groups | âœ… |
| POST | `/api/ai/chat` | Chat with RAG | âœ… |
| POST | `/api/ai/clear/{session_id}` | Clear session memory | âœ… |

### Job Endpoints

| Method | Endpoint | Purpose | Auth Required |
|--------|----------|---------|---|
| GET | `/api/jobs` | List user jobs | âœ… |
| GET | `/api/jobs/{job_id}` | Get job status | âœ… |

### Admin Endpoints

| Method | Endpoint | Purpose | Auth Required |
|--------|----------|---------|---|
| GET | `/api/admin/users` | List all users | âœ… Admin only |
| POST | `/api/admin/users` | Create user | âœ… Admin only |

---

## Frontend Components

### Main Views

1. **LoginView.vue**
   - Login form with username/password
   - Password validation
   - Error handling

2. **MainView.vue**
   - Tab navigation (Conversion, Comparison, Ask RET AI)
   - User info display
   - Logout button
   - Theme toggle

3. **AdminView.vue**
   - Admin dashboard
   - User management
   - System settings
   - Only visible to admin users

### Workspace Components

1. **ConversionPanel.vue**
   - ZIP file upload
   - Group detection display
   - Group selection checkboxes
   - Convert button
   - Download link

2. **ComparisonPanel.vue**
   - File A upload
   - File B upload
   - Compare button
   - Detailed results with delta indicators
   - Similarity percentage

3. **AIPanel.vue** (Ask RET AI)
   - ZIP upload or session selection
   - Available groups list
   - Index button
   - Indexed groups display
   - Chat input
   - Chat response with citations
   - Clear memory button

4. **BrandHeader.vue**
   - Logo and branding
   - Theme toggle (ğŸŒ“)
   - User info (top-right)
   - Admin button (admin only)
   - Logout button

---

## Database Schema

### Users Table
```sql
CREATE TABLE users (
  id UUID PRIMARY KEY,
  username VARCHAR(255) UNIQUE NOT NULL,
  email VARCHAR(255),
  hashed_password VARCHAR(255) NOT NULL,
  is_admin BOOLEAN DEFAULT false,
  is_active BOOLEAN DEFAULT true,
  created_at TIMESTAMP DEFAULT now(),
  updated_at TIMESTAMP DEFAULT now()
)
```

### Jobs Table
```sql
CREATE TABLE jobs (
  id UUID PRIMARY KEY,
  user_id UUID FOREIGN KEY,
  job_type VARCHAR(50),  -- 'conversion', 'comparison', 'indexing'
  status VARCHAR(50),    -- 'pending', 'processing', 'completed', 'failed'
  session_id VARCHAR(255),
  metadata JSONB,
  result JSONB,
  error_message TEXT,
  created_at TIMESTAMP DEFAULT now(),
  updated_at TIMESTAMP DEFAULT now(),
  completed_at TIMESTAMP
)
```

### Sessions Table
```sql
CREATE TABLE sessions (
  id VARCHAR(255) PRIMARY KEY,
  user_id UUID FOREIGN KEY,
  metadata JSONB,  -- {zip_name, groups, created_at}
  created_at TIMESTAMP DEFAULT now(),
  expires_at TIMESTAMP
)
```

### File Storage Structure
```
runtime/sessions/{session_id}/
â”œâ”€â”€ extracted/
â”‚   â”œâ”€â”€ journal_article_1.xml
â”‚   â”œâ”€â”€ journal_article_2.xml
â”‚   â”œâ”€â”€ book_1.xml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ JOURNAL.csv
â”‚   â”œâ”€â”€ BOOK.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ai_index/
â”‚   â”œâ”€â”€ chroma.sqlite
â”‚   â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ metadata/
â””â”€â”€ metadata.json
```

---

## External Integrations

### Azure OpenAI

**Configuration** (from .env):
```
AZURE_OPENAI_ENDPOINT=https://btgazureopenai.openai.azure.com/
AZURE_OPENAI_API_KEY=b7a813d6bdd8487c954991961f6d174e
AZURE_OPENAI_API_VERSION=2025-01-01-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=ESU_CBG_gpt-4.1
AZURE_OPENAI_EMBED_DEPLOYMENT=ESU_CBG_text-embedding-3-large
```

**Services**:
1. **Chat Model** (`gpt-4.1`):
   - Used for: AI chat responses, RAG answers
   - Temperature: 0.65 (moderate creativity)
   - Max tokens: auto

2. **Embeddings Model** (`text-embedding-3-large`):
   - Used for: Document indexing, semantic search
   - Output dimensions: 1536
   - Used by: Chroma DB for similarity search

**Cost Estimation**:
- Embeddings: ~$0.02 per 1M tokens
- Chat: ~$0.03 per 1K tokens (input), ~$0.06 per 1K tokens (output)
- Typical indexing (1000 docs): ~$1-2
- Typical chat query: ~$0.01

---

## Testing Overview

### Automated Tests

**File**: `test_all_features.py`

```bash
python test_all_features.py
```

Tests include:
- âœ… Backend health check
- âœ… Authentication (login, token refresh)
- âœ… File upload and ZIP scanning
- âœ… XML to CSV conversion
- âœ… File comparison
- âœ… AI indexing
- âœ… AI chat with RAG
- âœ… Admin features
- âœ… Session cleanup

### Manual Testing

**Guides**:
- `COMPREHENSIVE_TEST_GUIDE.md` - Detailed testing guide (6 sections)
- `MANUAL_TESTING_CHECKLIST.md` - Quick checklist

**Quick Start Test** (5 minutes):
1. Backend health: http://localhost:8000/health
2. Login: http://localhost:5173
3. Upload ZIP: Conversion tab
4. Index to AI: Ask RET AI tab
5. Ask question: Chat interface

### Test Data

Example files available in: `d:\WORK\RET_App\Examples\BIg_test-examples\`

Recommended sequence:
1. **Basic**: `journal_article_4.4.2.xml` (single file)
2. **Medium**: Mix of 3-5 different XML types
3. **Full**: All example files

---

## Feature Completion Status

### Phase 1: Core Features âœ… COMPLETE
- [x] Authentication & Authorization
- [x] File upload & ZIP processing
- [x] XML parsing and group detection
- [x] CSV/XLSX conversion
- [x] Session management

### Phase 2: Advanced Features âœ… COMPLETE
- [x] File comparison with delta analysis
- [x] Theme toggle (dark/light mode)
- [x] Admin panel with role-based access
- [x] Job status tracking
- [x] Token refresh automation

### Phase 3: AI/RAG Features âœ… COMPLETE
- [x] Azure OpenAI integration
- [x] Chroma vector DB setup
- [x] Document indexing to Chroma
- [x] Semantic search implementation
- [x] RAG-based chat with citations
- [x] Session-level memory management
- [x] Automatic cleanup on logout

### Phase 4: UI/UX âœ… COMPLETE
- [x] Responsive design
- [x] Tab-based navigation
- [x] Real-time job progress
- [x] Clear error messages
- [x] Loading indicators

---

## Performance Metrics

Based on 100 document indexing:

| Operation | Time | Notes |
|-----------|------|-------|
| ZIP scan | 1-2s | Fast (detection only) |
| Extraction | 2-5s | Depends on ZIP size |
| CSV conversion | 3-10s | Async job |
| Embeddings (100 docs) | 30-60s | Azure API calls |
| Semantic search | 100-200ms | Chroma DB query |
| Chat response | 2-5s | Including embeddings + GPT-4 |
| Total AI flow (Q&A) | 3-8s | End-to-end |

---

## Known Limitations & Future Enhancements

### Current Limitations:
- âš ï¸ Session data deleted on logout (not persistent)
- âš ï¸ Chroma DB uses SQLite (not suitable for production scale)
- âš ï¸ No document versioning
- âš ï¸ Single file comparison only

### Potential Enhancements:
- â˜ Persistent sessions with archival
- âš ï¸ Multi-file batch comparison
- â˜ Custom embeddings model fine-tuning
- â˜ Document metadata filtering in RAG
- â˜ Multi-language support
- â˜ Export/import session data
- â˜ Advanced analytics on conversions
- â˜ Webhook notifications

---

## Security Considerations

### Authentication Security
- âœ… Passwords: Argon2 hashing
- âœ… Tokens: JWT with HS256 signing
- âœ… Refresh: HttpOnly cookies, automatic rotation
- âœ… CORS: Properly configured, no open access

### Data Security
- âœ… Session isolation: Per-user directories
- âœ… Access control: User-specific session validation
- âœ… Clean-up: Complete deletion on logout
- âœ… No sensitive data in logs

### API Security
- âœ… Rate limiting: Middleware-based
- âœ… Input validation: Pydantic schemas
- âœ… File validation: ZIP bomb detection
- âœ… HTTPS ready: Settings support secure mode

---

## Troubleshooting

### Common Issues

**Issue**: Backend won't start
```
Solution:
1. Check Python 3.10+ installed: python --version
2. Activate venv: .venv\Scripts\Activate.ps1
3. Install deps: pip install -r requirements.txt
4. Check port: netstat -ano | findstr :8000
```

**Issue**: AI chat returns empty
```
Solution:
1. Check indexing completed successfully
2. Check Azure OpenAI credentials in .env
3. Check Chroma DB created: runtime/sessions/{id}/ai_index/
4. Check backend logs for API errors
```

**Issue**: Login fails
```
Solution:
1. Ensure database initialized: python scripts/init_db.py
2. Create demo users: python scripts/demo_users.py
3. Check DATABASE_URL in .env
4. Verify PostgreSQL running
```

---

## Support & Documentation

- **Quick Start**: See COMPREHENSIVE_TEST_GUIDE.md
- **API Docs**: http://localhost:8000/docs (Swagger UI)
- **Testing**: Run test_all_features.py
- **Source Code**: See backend/api/ and frontend/src/

---

**Last Updated**: January 27, 2026  
**Maintained By**: Development Team  
**Version**: 4.0.0
